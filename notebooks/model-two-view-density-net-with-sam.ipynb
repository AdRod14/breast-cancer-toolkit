{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac0cf9d-f4ba-4046-9277-9ce4b267548b",
   "metadata": {},
   "source": [
    "# CNN with SAM\n",
    "M. Busaleh, M. Hussain, H. A. Aboalsamh, Fazal-e-Amin, and S. A. Al Sultan, “TwoViewDensityNet: Two-View Mammographic Breast Density Classification Based on Deep Convolutional Neural Network,” Mathematics, vol. 10, no. 23, p. 4610, Dec. 2022, doi: 10.3390/math10234610."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb8c5-eae7-456c-bb89-31adf4b37ba2",
   "metadata": {},
   "source": [
    "# Viewer \n",
    "\n",
    "\n",
    "## Observations\n",
    "\n",
    "1. Images were transformed to 8-bit gray.\n",
    "2. Preprocessing does  wrong its job in some the images below\n",
    "\n",
    "A_1708_1.LEFT_MLO.tif\n",
    "\n",
    "A_1500_1.LEFT.MLO.tif\n",
    "\n",
    "A_1980_1.LEFT_CC.tif\n",
    "\n",
    "B_3608.LEFT_MLO.tif\n",
    "\n",
    "B3490_1.RIGHT_MLO.tif\n",
    "\n",
    "A_1701_RIGHT.RIGHT_CC.tf\n",
    "\n",
    "A_1425_1_RIGHT_MLO.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aba03-45f6-4d27-95a6-11670ecd57ff",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8291d-4290-4f71-9017-e7fbe514c533",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8dc8be2-7e88-4013-b03d-f0a56150387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (2.8.0)\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ba/7c/b971f2485155917ecdcebb210e021e36a6b65457394590be01cc61515310/tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/40/fa/98115f6fe4d92e1962f549917be2dc8e369853b7e404191996fedaaf4dd6/tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading numpy-1.24.3-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "     --------------------------------------- 14.8/14.8 MB 14.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/5e/46/5b9674a33cbf690ffdd79ab1863767a66461cd06ea7aeb9f90e4e50be7a5/protobuf-4.24.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Using cached protobuf-4.24.3-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.57.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 14.8 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.27.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "   --------------------------------------- 276.5/276.5 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.4 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.24.3-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 430.5/430.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "   --------------------------------------- 440.8/440.8 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: typing-extensions, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, keras, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.10.0\n",
      "    Uninstalling tensorflow-estimator-2.10.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -reast-cancer-toolkit (c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -reast-cancer-toolkit (c:\\users\\uabc_\\anaconda3\\envs\\env3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\uabc_\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-zod3wtkt\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca20543f-2bb3-4611-9f4a-0f2a4a08e290",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m;\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal([\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1000\u001b[39m])))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca416916-1bce-40d5-8beb-39fc5ba1d11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121053fe-1828-4b62-bc31-be0fede51685",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\uabc_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "K = 4 # number of classses\n",
    "N = 32 # number of training instances in a batch.\n",
    "shape = (336, 224, 3)\n",
    "\n",
    "# y_true has the shape (batch_size, d0, ... dN)\n",
    "# y_pred has the shape (batch_size, d0, .. dN)\n",
    "# Example\n",
    "# y_true = [0, 1, 2]\n",
    "# y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]]\n",
    "def sparse_categorical_weighted_cross_entropy_loss(y_true, y_pred):\n",
    "    # calculate_class_weights\n",
    "    y, _, count = tf.unique_with_counts(y_true[:,0])\n",
    "    label_space = tf.constant(y_pred.shape[1])\n",
    "    output = tf.zeros(label_space, dtype=count.dtype)\n",
    "    output = tf.tensor_scatter_nd_update(output, tf.expand_dims(y, axis=1)-1, count)\n",
    "    class_weights_tensor =  tf.cast(output/len(y_true), dtype=tf.float64)\n",
    "    # 10^i <= y_{ni} <= 1-10^i\n",
    "    y_pred = tf.cast(tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7), dtype=tf.float64)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float64)\n",
    "    return -tf.reduce_sum(class_weights_tensor * y_true * tf.math.log(y_pred), axis=-1)/len(y_true)\n",
    "\n",
    "def sparse_categorical_weighted_cross_entropy_loss_with_logits(y_true, y_pred):\n",
    "  y_true_one_hot = tf.one_hot(y_true[:,0], 5)\n",
    "  sparse_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_one_hot, logits=y_pred)\n",
    "  return sparse_ce\n",
    "\n",
    "# f = SparseCategoricalCrossentropy(from_logits=False,reduction=tf.keras.losses.Reduction.NONE)\n",
    "# f = SparseCategoricalFocalLoss(gamma=0.25,reduction=tf.keras.losses.Reduction.NONE)\n",
    "# loss_fn = sparse_categorical_weighted_cross_entropy_loss\n",
    "# f = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def load(path):\n",
    "    with open(f\"mammograms_npy\\\\{path}.npy\", 'rb') as file:\n",
    "        return tf.convert_to_tensor(np.load(file)*1./255)\n",
    "\n",
    "def generate_from_df(index):\n",
    "  for (index,instance) in df.iloc[index].iterrows():\n",
    "    yield (load(instance['minio_path'][0]),load(instance['minio_path'][1])), instance['breast_density'][0]\n",
    "\n",
    "def generate_dataset_from_index(index):\n",
    "    return tf.data.Dataset.from_generator(\n",
    "       generate_from_df, \n",
    "       args=[index],\n",
    "       output_signature=(\n",
    "         (tf.TensorSpec(shape=shape, dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=shape, dtype=tf.int32)),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        )\n",
    "      ).batch(32)\n",
    "\n",
    "def create_model(loss_fn):\n",
    "    \"\"\"\n",
    "    The model below is a ResNet-50 pretrained on ImageNet [27];\n",
    "    its architecture is based on residual learning, which allows increasing the depth of a CNN\n",
    "    model that prevents the problem of gradient vanishing [31] and degradation [32,33].\n",
    "    Text from Paper.\n",
    "    \"\"\"\n",
    "    res_net1 = ResNet50(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    res_net2 = ResNet50(weights='imagenet', include_top=False, input_shape=shape)\n",
    "\n",
    "    base_model1 = Model(res_net1.input, res_net1.output, name='ResNet50_View1')\n",
    "    base_model2 = Model(res_net2.input, res_net2.output, name='ResNet50_View2')\n",
    "\n",
    "    for layer in base_model1.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in base_model2.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    input1 = Input(shape=shape)\n",
    "    input2 = Input(shape=shape)\n",
    "\n",
    "    features1 = base_model1(input1)\n",
    "    features2 = base_model2(input2)\n",
    "\n",
    "    flattened_features1 = Flatten()(features1)\n",
    "    flattened_features2 = Flatten()(features2)\n",
    "\n",
    "    \"\"\"\n",
    "    In our proposed method, the features from the two views are fused by the concatenation layer.\n",
    "    Text from Paper.\n",
    "    \"\"\"\n",
    "    concatenated = tf.keras.layers.concatenate([flattened_features1, flattened_features2])\n",
    "\n",
    "    \"\"\"\n",
    "    The last layer of the model is the classification layer; it is a fully connected layer with\n",
    "    four output neurons to classify the input views into one of the four breast density categories;\n",
    "    each neuron represents a different BI-RADS class.\n",
    "\n",
    "    SoftMax converts the numerical output of a convolutional neural network to class-specific probability values.\n",
    "    \"\"\"\n",
    "    output = Dense(K, activation='softmax')(concatenated)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=10**-4, momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(tf.keras.losses.mean_squared_error)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be1b10c-9c49-46c1-982c-8d25de85b122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6a049-7318-4892-aa1a-47808915aba9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd56e227-ee22-41dd-a729-eb9b52147cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>index</th>\n",
       "      <th>breast_density</th>\n",
       "      <th>side</th>\n",
       "      <th>view</th>\n",
       "      <th>assessment</th>\n",
       "      <th>pathology</th>\n",
       "      <th>subtlety</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>od_img_path</th>\n",
       "      <th>minio_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-0243-1</td>\n",
       "      <td>[2054, 2057]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[MLO, CC]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[3256, 3196]</td>\n",
       "      <td>[5446, 5461]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[A_0243_1.RIGHT_CC.tif, A_0243_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-0244-1</td>\n",
       "      <td>[1990, 1992]</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[1861, 2011]</td>\n",
       "      <td>[3886, 4876]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[A_0244_1.RIGHT_CC.tif, A_0244_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-0245-1</td>\n",
       "      <td>[2328, 2329]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[2371, 2296]</td>\n",
       "      <td>[4561, 4726]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[A_0245_1.RIGHT_CC.tif, A_0245_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-0246-1</td>\n",
       "      <td>[2284, 2285]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[2146, 2266]</td>\n",
       "      <td>[4891, 5071]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[A_0246_1.RIGHT_CC.tif, A_0246_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-0247-1</td>\n",
       "      <td>[1970, 1973]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[2806, 2956]</td>\n",
       "      <td>[5446, 5431]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[A_0247_1.RIGHT_CC.tif, A_0247_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>D-4605-1</td>\n",
       "      <td>[1636, 1637]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[4111, 4801]</td>\n",
       "      <td>[6601, 6586]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[D_4605_1.RIGHT_CC.tif, D_4605_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>D-4606-1</td>\n",
       "      <td>[1618, 1620]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[MLO, CC]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[3796, 3736]</td>\n",
       "      <td>[6331, 6241]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[D_4606_1.RIGHT_CC.tif, D_4606_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>D-4607-1</td>\n",
       "      <td>[1667, 1669]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[MLO, CC]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[4426, 4231]</td>\n",
       "      <td>[6541, 6811]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[D_4607_1.RIGHT_CC.tif, D_4607_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>D-4608-1</td>\n",
       "      <td>[1615, 1617]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[MLO, CC]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[3976, 3961]</td>\n",
       "      <td>[6691, 5941]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[D_4608_1.RIGHT_CC.tif, D_4608_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>D-4609-1</td>\n",
       "      <td>[1650, 1653]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[RIGHT, RIGHT]</td>\n",
       "      <td>[CC, MLO]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[normals, normals]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[3031, 3436]</td>\n",
       "      <td>[6181, 6556]</td>\n",
       "      <td>[/home/cicese/condaProj/DDSM-ori/cases/process...</td>\n",
       "      <td>[D_4609_1.RIGHT_CC.tif, D_4609_1.RIGHT_MLO.tif]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2434 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id         index breast_density            side       view  \\\n",
       "0      A-0243-1  [2054, 2057]         [2, 2]  [RIGHT, RIGHT]  [MLO, CC]   \n",
       "1      A-0244-1  [1990, 1992]         [3, 3]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "2      A-0245-1  [2328, 2329]         [4, 4]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "3      A-0246-1  [2284, 2285]         [2, 2]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "4      A-0247-1  [1970, 1973]         [1, 1]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "...         ...           ...            ...             ...        ...   \n",
       "2429   D-4605-1  [1636, 1637]         [2, 2]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "2430   D-4606-1  [1618, 1620]         [2, 2]  [RIGHT, RIGHT]  [MLO, CC]   \n",
       "2431   D-4607-1  [1667, 1669]         [1, 1]  [RIGHT, RIGHT]  [MLO, CC]   \n",
       "2432   D-4608-1  [1615, 1617]         [2, 2]  [RIGHT, RIGHT]  [MLO, CC]   \n",
       "2433   D-4609-1  [1650, 1653]         [2, 2]  [RIGHT, RIGHT]  [CC, MLO]   \n",
       "\n",
       "      assessment           pathology    subtlety         width        height  \\\n",
       "0     [nan, nan]  [normals, normals]  [nan, nan]  [3256, 3196]  [5446, 5461]   \n",
       "1     [nan, nan]  [normals, normals]  [nan, nan]  [1861, 2011]  [3886, 4876]   \n",
       "2     [nan, nan]  [normals, normals]  [nan, nan]  [2371, 2296]  [4561, 4726]   \n",
       "3     [nan, nan]  [normals, normals]  [nan, nan]  [2146, 2266]  [4891, 5071]   \n",
       "4     [nan, nan]  [normals, normals]  [nan, nan]  [2806, 2956]  [5446, 5431]   \n",
       "...          ...                 ...         ...           ...           ...   \n",
       "2429  [nan, nan]  [normals, normals]  [nan, nan]  [4111, 4801]  [6601, 6586]   \n",
       "2430  [nan, nan]  [normals, normals]  [nan, nan]  [3796, 3736]  [6331, 6241]   \n",
       "2431  [nan, nan]  [normals, normals]  [nan, nan]  [4426, 4231]  [6541, 6811]   \n",
       "2432  [nan, nan]  [normals, normals]  [nan, nan]  [3976, 3961]  [6691, 5941]   \n",
       "2433  [nan, nan]  [normals, normals]  [nan, nan]  [3031, 3436]  [6181, 6556]   \n",
       "\n",
       "                                            od_img_path  \\\n",
       "0     [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "1     [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "2     [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "3     [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "4     [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "...                                                 ...   \n",
       "2429  [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "2430  [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "2431  [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "2432  [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "2433  [/home/cicese/condaProj/DDSM-ori/cases/process...   \n",
       "\n",
       "                                           minio_path  \n",
       "0     [A_0243_1.RIGHT_CC.tif, A_0243_1.RIGHT_MLO.tif]  \n",
       "1     [A_0244_1.RIGHT_CC.tif, A_0244_1.RIGHT_MLO.tif]  \n",
       "2     [A_0245_1.RIGHT_CC.tif, A_0245_1.RIGHT_MLO.tif]  \n",
       "3     [A_0246_1.RIGHT_CC.tif, A_0246_1.RIGHT_MLO.tif]  \n",
       "4     [A_0247_1.RIGHT_CC.tif, A_0247_1.RIGHT_MLO.tif]  \n",
       "...                                               ...  \n",
       "2429  [D_4605_1.RIGHT_CC.tif, D_4605_1.RIGHT_MLO.tif]  \n",
       "2430  [D_4606_1.RIGHT_CC.tif, D_4606_1.RIGHT_MLO.tif]  \n",
       "2431  [D_4607_1.RIGHT_CC.tif, D_4607_1.RIGHT_MLO.tif]  \n",
       "2432  [D_4608_1.RIGHT_CC.tif, D_4608_1.RIGHT_MLO.tif]  \n",
       "2433  [D_4609_1.RIGHT_CC.tif, D_4609_1.RIGHT_MLO.tif]  \n",
       "\n",
       "[2434 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "file_names = glob.glob(\"experiment-0-preprocessed-mammograms/*.npy\")\n",
    "\n",
    "prefixes = [filename.split('\\\\')[1].replace(\".npy\", \"\") for filename in file_names]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def encode_columns(row):\n",
    "    return row['patient_id'].replace('-', '_') + '.'+row['side']+'_'+row['view']+'.'+'tif'\n",
    "\n",
    "cases = pd.read_csv('http://127.0.0.1:9000/ddsm/ddsm_description_cases.csv')\n",
    "cases['minio_path'] = cases.apply(encode_columns, axis=1)\n",
    "correct_cases = cases[cases['minio_path'].apply(lambda path: path in prefixes)]\n",
    "correct_cases = correct_cases[correct_cases['breast_density'].apply(lambda instance: instance != 0)]\n",
    "correct_cases = correct_cases.reset_index()\n",
    "df = correct_cases[correct_cases[\"minio_path\"].apply(lambda path: \"RIGHT\" in path)].groupby(\"patient_id\").agg(list)\n",
    "df['minio_path'] = df['minio_path'].apply(lambda paths: sorted(list(set(paths))))\n",
    "df = df[df['minio_path'].map(len) >= 2]\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc0c1c-3b3c-4543-a099-2474e285fe9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>breast_density</th>\n",
       "      <th>side</th>\n",
       "      <th>view</th>\n",
       "      <th>assessment</th>\n",
       "      <th>pathology</th>\n",
       "      <th>subtlety</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>od_img_path</th>\n",
       "      <th>minio_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>A-0121-1</td>\n",
       "      <td>2</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2654</td>\n",
       "      <td>5519</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0121_1.LEFT_CC.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>A-0121-1</td>\n",
       "      <td>2</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2639</td>\n",
       "      <td>5474</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0121_1.LEFT_MLO.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>A-0121-1</td>\n",
       "      <td>2</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2729</td>\n",
       "      <td>5504</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0121_1.RIGHT_MLO.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>A-0121-1</td>\n",
       "      <td>2</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2804</td>\n",
       "      <td>5474</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0121_1.RIGHT_CC.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>A-0070-1</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2609</td>\n",
       "      <td>5264</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0070_1.LEFT_MLO.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>A-0223-1</td>\n",
       "      <td>2</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2176</td>\n",
       "      <td>4516</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_0223_1.RIGHT_CC.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8257</th>\n",
       "      <td>A-1825-1</td>\n",
       "      <td>0</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1816</td>\n",
       "      <td>4501</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_1825_1.LEFT_CC.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>A-1825-1</td>\n",
       "      <td>0</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2671</td>\n",
       "      <td>5491</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_1825_1.LEFT_MLO.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>A-1825-1</td>\n",
       "      <td>0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1831</td>\n",
       "      <td>4576</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_1825_1.RIGHT_CC.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>A-1825-1</td>\n",
       "      <td>0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2716</td>\n",
       "      <td>5176</td>\n",
       "      <td>/home/cicese/condaProj/DDSM-ori/cases/processe...</td>\n",
       "      <td>A_1825_1.RIGHT_MLO.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id  breast_density   side view  assessment pathology  subtlety  \\\n",
       "934    A-0121-1               2   LEFT   CC         NaN   normals       NaN   \n",
       "935    A-0121-1               2   LEFT  MLO         NaN   normals       NaN   \n",
       "936    A-0121-1               2  RIGHT  MLO         NaN   normals       NaN   \n",
       "937    A-0121-1               2  RIGHT   CC         NaN   normals       NaN   \n",
       "938    A-0070-1               3   LEFT  MLO         NaN   normals       NaN   \n",
       "...         ...             ...    ...  ...         ...       ...       ...   \n",
       "2377   A-0223-1               2  RIGHT   CC         NaN   normals       NaN   \n",
       "8257   A-1825-1               0   LEFT   CC         NaN   cancers       NaN   \n",
       "8258   A-1825-1               0   LEFT  MLO         NaN   cancers       NaN   \n",
       "8259   A-1825-1               0  RIGHT   CC         NaN   cancers       NaN   \n",
       "8260   A-1825-1               0  RIGHT  MLO         NaN   cancers       NaN   \n",
       "\n",
       "      width  height                                        od_img_path  \\\n",
       "934    2654    5519  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "935    2639    5474  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "936    2729    5504  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "937    2804    5474  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "938    2609    5264  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "...     ...     ...                                                ...   \n",
       "2377   2176    4516  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "8257   1816    4501  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "8258   2671    5491  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "8259   1831    4576  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "8260   2716    5176  /home/cicese/condaProj/DDSM-ori/cases/processe...   \n",
       "\n",
       "                  minio_path  \n",
       "934     A_0121_1.LEFT_CC.tif  \n",
       "935    A_0121_1.LEFT_MLO.tif  \n",
       "936   A_0121_1.RIGHT_MLO.tif  \n",
       "937    A_0121_1.RIGHT_CC.tif  \n",
       "938    A_0070_1.LEFT_MLO.tif  \n",
       "...                      ...  \n",
       "2377   A_0223_1.RIGHT_CC.tif  \n",
       "8257    A_1825_1.LEFT_CC.tif  \n",
       "8258   A_1825_1.LEFT_MLO.tif  \n",
       "8259   A_1825_1.RIGHT_CC.tif  \n",
       "8260  A_1825_1.RIGHT_MLO.tif  \n",
       "\n",
       "[608 rows x 11 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_cases = cases[cases['patient_id'].apply(lambda patient_id: not correct_cases['patient_id'].isin([patient_id]).any() )]\n",
    "wrong_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19766078-8b86-4ecc-916e-315623c9a53e",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e4131f-1df7-49a6-bc0a-0d9bd94f7170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['breast_density'].apply(lambda instance: instance[0]).hist(grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae204c6-adac-4fab-be13-3b92a812b44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    332\n",
       "2    931\n",
       "3    710\n",
       "4    461\n",
       "Name: breast_density, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density= df['breast_density'].apply(lambda instance: instance[0])\n",
    "density.groupby(lambda x: density[x]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e32cb-0c84-4345-9e9f-b1a2057fb7f3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8db5d55d-a88d-434d-b0b5-94e4f9a5f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e2c534b-f9c3-43bc-8cc3-5a2149bb5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def train(loss_fn,number_of_folds=5,epochs=50,batch_size=64):\n",
    "    k = number_of_folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    fold_no = 1\n",
    "    for train_index, val_index in kf.split(df):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        train_dataset = generate_dataset_from_index(train_index).map(lambda inputs, label: ((data_augmentation(inputs[0], training=True),data_augmentation(inputs[1], training=True)), label), num_parallel_calls=AUTOTUNE)\n",
    "        val_dataset = generate_dataset_from_index(val_index)\n",
    "        model = create_model(loss_fn)\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=[tensorboard_callback]\n",
    "        )\n",
    "        fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435156e9-5d3b-4184-8549-cbe1358bbb61",
   "metadata": {},
   "source": [
    "## Loss function: Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1945d7bf-794e-4442-8da2-8811663f17a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 322ms/step - loss: 6.1709 - accuracy: 0.1834 - val_loss: 6.0289 - val_accuracy: 0.1396\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1658 - accuracy: 0.1875 - val_loss: 6.0289 - val_accuracy: 0.3922\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 6.1657 - accuracy: 0.1870 - val_loss: 6.0288 - val_accuracy: 0.3922\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 6.1632 - accuracy: 0.1870 - val_loss: 6.0132 - val_accuracy: 0.3922\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1456 - accuracy: 0.2173 - val_loss: 6.0081 - val_accuracy: 0.3922\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 6.1449 - accuracy: 0.2655 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 6.1449 - accuracy: 0.2625 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 20s 317ms/step - loss: 6.1449 - accuracy: 0.1988 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 6.1449 - accuracy: 0.1793 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 6.1449 - accuracy: 0.1895 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 6.1449 - accuracy: 0.2018 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: 6.1449 - accuracy: 0.2060 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 6.1449 - accuracy: 0.1715 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: 6.1449 - accuracy: 0.1936 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 6.1449 - accuracy: 0.1977 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.1449 - accuracy: 0.1875 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2080 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1449 - accuracy: 0.1777 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2024 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.1449 - accuracy: 0.1870 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1449 - accuracy: 0.2131 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1449 - accuracy: 0.2008 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.1449 - accuracy: 0.2070 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2316 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1449 - accuracy: 0.2286 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2835 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2650 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 6.1449 - accuracy: 0.2465 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1449 - accuracy: 0.2573 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1449 - accuracy: 0.2614 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 6.1449 - accuracy: 0.2578 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2578 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2696 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 6.1449 - accuracy: 0.1988 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1449 - accuracy: 0.2244 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2388 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1449 - accuracy: 0.2373 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.1449 - accuracy: 0.2152 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2008 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1449 - accuracy: 0.2460 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1449 - accuracy: 0.1782 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2111 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2152 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1449 - accuracy: 0.2445 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1449 - accuracy: 0.2280 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.1449 - accuracy: 0.2075 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 6.1449 - accuracy: 0.1957 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2368 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1449 - accuracy: 0.2322 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 6.1449 - accuracy: 0.2250 - val_loss: 6.0081 - val_accuracy: 0.2854\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 314ms/step - loss: 6.0686 - accuracy: 0.2583 - val_loss: 6.3202 - val_accuracy: 0.1211\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.2270 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.0669 - accuracy: 0.1941 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1710 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1546 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.0669 - accuracy: 0.1582 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1644 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1587 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.0669 - accuracy: 0.1828 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.0669 - accuracy: 0.1695 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1875 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1977 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.0669 - accuracy: 0.1890 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 6.0669 - accuracy: 0.1674 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 6.0669 - accuracy: 0.1448 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 6.0669 - accuracy: 0.1597 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 6.0669 - accuracy: 0.1649 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 6.0669 - accuracy: 0.1649 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 6.0669 - accuracy: 0.1489 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 20s 317ms/step - loss: 6.0669 - accuracy: 0.1926 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 6.0669 - accuracy: 0.2229 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 6.0669 - accuracy: 0.2229 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1977 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1587 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.0669 - accuracy: 0.1397 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1577 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.0669 - accuracy: 0.1392 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1222 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.0669 - accuracy: 0.1644 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 18s 296ms/step - loss: 6.0669 - accuracy: 0.1644 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1690 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.0669 - accuracy: 0.1613 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 18s 296ms/step - loss: 6.0669 - accuracy: 0.2013 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1741 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.0669 - accuracy: 0.1859 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1854 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 18s 296ms/step - loss: 6.0669 - accuracy: 0.1715 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.0669 - accuracy: 0.2275 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 18s 296ms/step - loss: 6.0669 - accuracy: 0.2291 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1859 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1808 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.0669 - accuracy: 0.1608 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1536 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.0669 - accuracy: 0.1561 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.0669 - accuracy: 0.2018 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.0669 - accuracy: 0.1710 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.0669 - accuracy: 0.1772 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.0669 - accuracy: 0.1844 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1787 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.0669 - accuracy: 0.1654 - val_loss: 6.3202 - val_accuracy: 0.3244\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 316ms/step - loss: 6.1275 - accuracy: 0.1669 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1572 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1669 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1257 - accuracy: 0.1926 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1257 - accuracy: 0.1823 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1967 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1257 - accuracy: 0.1931 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1257 - accuracy: 0.1962 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1798 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.2003 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1257 - accuracy: 0.2018 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1257 - accuracy: 0.2090 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.2419 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1257 - accuracy: 0.2260 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1257 - accuracy: 0.2162 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1895 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1257 - accuracy: 0.1782 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1257 - accuracy: 0.1983 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1257 - accuracy: 0.1911 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1257 - accuracy: 0.2090 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.1983 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.1926 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.2126 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.1977 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 6.1257 - accuracy: 0.1798 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.1710 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 6.1257 - accuracy: 0.2440 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.2060 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1257 - accuracy: 0.2060 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1257 - accuracy: 0.1700 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1962 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1257 - accuracy: 0.1638 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1257 - accuracy: 0.1900 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1813 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1823 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1257 - accuracy: 0.1751 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1257 - accuracy: 0.1638 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1875 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1257 - accuracy: 0.1834 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1916 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 18s 296ms/step - loss: 6.1257 - accuracy: 0.1972 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1257 - accuracy: 0.1998 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1257 - accuracy: 0.1864 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1257 - accuracy: 0.1705 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1315 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1257 - accuracy: 0.1459 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1257 - accuracy: 0.1787 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1669 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1257 - accuracy: 0.1721 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1257 - accuracy: 0.1525 - val_loss: 6.0851 - val_accuracy: 0.3676\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 323ms/step - loss: 6.1409 - accuracy: 0.2342 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1390 - accuracy: 0.2399 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2630 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1390 - accuracy: 0.2173 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1390 - accuracy: 0.2388 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2209 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2476 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.1993 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.1957 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2085 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2373 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1390 - accuracy: 0.2162 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.1988 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2162 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1390 - accuracy: 0.2332 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2399 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2470 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1390 - accuracy: 0.2096 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2671 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2686 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2573 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1390 - accuracy: 0.2625 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2589 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2886 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1390 - accuracy: 0.2676 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2419 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2681 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 6.1390 - accuracy: 0.2465 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1390 - accuracy: 0.2625 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1390 - accuracy: 0.2748 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1390 - accuracy: 0.2835 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 20s 322ms/step - loss: 6.1390 - accuracy: 0.3051 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1390 - accuracy: 0.2542 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 6.1390 - accuracy: 0.2666 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1390 - accuracy: 0.2717 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 6.1390 - accuracy: 0.2558 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1390 - accuracy: 0.2604 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2743 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2491 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 6.1390 - accuracy: 0.2157 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2661 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2604 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1390 - accuracy: 0.2717 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2501 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2548 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1390 - accuracy: 0.2917 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 6.1390 - accuracy: 0.2964 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1390 - accuracy: 0.2537 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1390 - accuracy: 0.2645 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1390 - accuracy: 0.2558 - val_loss: 6.0317 - val_accuracy: 0.2608\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 313ms/step - loss: 6.1242 - accuracy: 0.2177 - val_loss: 6.1430 - val_accuracy: 0.1296\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1776 - val_loss: 6.1427 - val_accuracy: 0.1337\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1113 - accuracy: 0.2372 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.2048 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1797 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1113 - accuracy: 0.2038 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.2053 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1473 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1113 - accuracy: 0.1956 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1966 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.2136 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1113 - accuracy: 0.1668 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1715 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1858 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1807 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1113 - accuracy: 0.1946 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.2156 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1884 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1113 - accuracy: 0.2012 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1571 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1704 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1113 - accuracy: 0.1412 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1129 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1113 - accuracy: 0.1355 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 6.1113 - accuracy: 0.1520 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1478 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1935 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.1648 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 6.1113 - accuracy: 0.1797 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1751 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1894 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 6.1113 - accuracy: 0.1674 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.2325 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.1766 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 6.1113 - accuracy: 0.1817 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 6.1113 - accuracy: 0.1417 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1113 - accuracy: 0.2017 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 6.1113 - accuracy: 0.1910 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1113 - accuracy: 0.1792 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1113 - accuracy: 0.1756 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 6.1113 - accuracy: 0.1869 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1113 - accuracy: 0.1602 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 6.1113 - accuracy: 0.1694 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 6.1113 - accuracy: 0.1761 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: 6.1113 - accuracy: 0.1586 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 6.1113 - accuracy: 0.2023 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 6.1113 - accuracy: 0.2490 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 6.1113 - accuracy: 0.2669 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: 6.1113 - accuracy: 0.2931 - val_loss: 6.1427 - val_accuracy: 0.0041\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 18s 297ms/step - loss: 6.1113 - accuracy: 0.2592 - val_loss: 6.1427 - val_accuracy: 0.0041\n"
     ]
    }
   ],
   "source": [
    "train(loss_fn=tf.keras.losses.mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a1e0a-3c0c-4799-ad06-3d123f558b34",
   "metadata": {},
   "source": [
    "## Sum square error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8db01-a842-4314-a315-c320ad357e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18ed5c-2016-4f8f-9b51-e922f1e24405",
   "metadata": {},
   "source": [
    "## Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30072e35-ee5c-46d0-87ab-44c24cc7c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 315ms/step - loss: nan - accuracy: 0.0067 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 20s 326ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 20s 322ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 316ms/step - loss: nan - accuracy: 0.0046 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 329ms/step - loss: nan - accuracy: 0.0062 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 20s 317ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 332ms/step - loss: nan - accuracy: 0.0026 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 323ms/step - loss: nan - accuracy: 0.0046 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 18s 298ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "train(loss_fn=SparseCategoricalFocalLoss(gamma=0.25,reduction=tf.keras.losses.Reduction.NONE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d65c8-b305-40a7-8464-cebcc0486777",
   "metadata": {},
   "source": [
    "## Sparse Weighted cross-entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2102526a-8e50-4ee9-8f1b-7ed1d566e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 321ms/step - loss: 0.1117 - accuracy: 0.2209 - val_loss: 0.1090 - val_accuracy: 0.1437\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 0.1184 - accuracy: 0.2527 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 20s 329ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 20s 331ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 20s 330ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 20s 327ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1177 - accuracy: 0.2486 - val_loss: 0.1094 - val_accuracy: 0.1437\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 322ms/step - loss: 0.1204 - accuracy: 0.2244 - val_loss: 0.1388 - val_accuracy: 0.1170\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 0.1179 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 20s 333ms/step - loss: 0.1176 - accuracy: 0.2512 - val_loss: 0.1413 - val_accuracy: 0.1170\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1173 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 21s 341ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 20s 325ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1172 - accuracy: 0.2512 - val_loss: 0.1414 - val_accuracy: 0.1170\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 316ms/step - loss: 0.1283 - accuracy: 0.2568 - val_loss: 0.1165 - val_accuracy: 0.1520\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1148 - accuracy: 0.2368 - val_loss: 0.1205 - val_accuracy: 0.1520\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1163 - accuracy: 0.2419 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 20s 329ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 18s 300ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 20s 322ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 21s 333ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 20s 328ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 20s 327ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 20s 318ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1209 - val_accuracy: 0.1520\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 313ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 21s 336ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1160 - accuracy: 0.2388 - val_loss: 0.1208 - val_accuracy: 0.1520\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 23s 322ms/step - loss: 0.1289 - accuracy: 0.2583 - val_loss: 0.1222 - val_accuracy: 0.1437\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1118 - accuracy: 0.2275 - val_loss: 0.1203 - val_accuracy: 0.1437\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1100 - accuracy: 0.2342 - val_loss: 0.1193 - val_accuracy: 0.1437\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 20s 317ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 18s 301ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 301ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 20s 322ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 20s 320ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 20s 323ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 20s 317ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 312ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 314ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 317ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 20s 319ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 20s 322ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 21s 335ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 21s 349ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 20s 326ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 20s 325ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1193 - val_accuracy: 0.1437\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1194 - val_accuracy: 0.1437\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1193 - val_accuracy: 0.1437\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1193 - val_accuracy: 0.1437\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1101 - accuracy: 0.2342 - val_loss: 0.1193 - val_accuracy: 0.1437\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 22s 315ms/step - loss: 0.1360 - accuracy: 0.2454 - val_loss: 0.1266 - val_accuracy: 0.1255\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1125 - accuracy: 0.2346 - val_loss: 0.1259 - val_accuracy: 0.1255\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 19s 302ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 19s 305ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 19s 304ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 19s 308ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 19s 310ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 19s 303ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 19s 311ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 19s 307ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 19s 316ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 20s 328ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 21s 335ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 23s 376ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 21s 347ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 23s 381ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 22s 364ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 23s 371ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 22s 357ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 22s 352ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 22s 353ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 21s 334ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 20s 329ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1261 - val_accuracy: 0.1255\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 21s 335ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 21s 334ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 21s 348ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 21s 339ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 21s 341ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 21s 336ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 20s 331ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 20s 328ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 20s 327ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 21s 334ms/step - loss: 0.1123 - accuracy: 0.2341 - val_loss: 0.1260 - val_accuracy: 0.1255\n"
     ]
    }
   ],
   "source": [
    "train(loss_fn=sparse_categorical_weighted_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bb447-456b-40de-9ecc-5a08afc3c293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
